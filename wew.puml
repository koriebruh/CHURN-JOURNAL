@startuml
!theme plain
skinparam backgroundColor #F8F9FA
skinparam activity {
    BackgroundColor #E3F2FD
    BorderColor #1976D2
    FontSize 11
    FontColor #263238
}
skinparam decision {
    BackgroundColor #FFF3E0
    BorderColor #F57C00
    FontSize 11
    FontColor #263238
}
skinparam note {
    BackgroundColor #F1F8E9
    BorderColor #689F38
    FontSize 10
}

title **Customer Churn Prediction Research Methodology**

start

:üìä **STEP 1: Data Collection & Loading**
„ÉªLoad Telco Customer Churn Dataset
„ÉªValidate dataset integrity
„ÉªCheck for file existence and format;

note right
**Dataset Requirements:**
‚Ä¢ Customer demographics
‚Ä¢ Service usage patterns
‚Ä¢ Billing information
‚Ä¢ Churn labels (target variable)
end note

:üîç **STEP 2: Exploratory Data Analysis**
„ÉªAnalyze dataset structure and dimensions
„ÉªExamine missing values distribution
„ÉªInvestigate target variable balance
„ÉªGenerate descriptive statistics;

note left
**Key Metrics:**
‚Ä¢ Dataset shape: samples √ó features
‚Ä¢ Missing value percentage
‚Ä¢ Class distribution (Churn vs No-Churn)
‚Ä¢ Feature data types
end note

:üõ†Ô∏è **STEP 3: Data Preprocessing**
„ÉªNormalize column names
„ÉªHandle missing values (TotalCharges)
„ÉªConvert data types appropriately
„ÉªRemove irrelevant features (customerID);

:‚öôÔ∏è **STEP 4: Feature Engineering**
„ÉªCreate Engagement_Score
„ÉªDevelop Churn_Risk_Score
„ÉªCalculate Service_Utilization
„ÉªGenerate Payment_Reliability indicator
„ÉªApply one-hot encoding for categorical variables;

note right
**New Features:**
‚Ä¢ Engagement_Score = tenure √ó MonthlyCharges / (TotalCharges + 1)
‚Ä¢ Churn_Risk_Score = (tenure < 12) √ó MonthlyCharges
‚Ä¢ Service_Utilization = count of active services
‚Ä¢ Payment_Reliability = automatic payment flag
end note

:üìà **STEP 5: Feature Analysis**
„ÉªGenerate correlation matrix
„ÉªAnalyze feature relationships
„ÉªIdentify multicollinearity issues
„ÉªCreate visualization plots;

:‚öñÔ∏è **STEP 6: Data Balancing**
„ÉªApply StandardScaler for normalization
„ÉªCompare multiple sampling techniques:
  ‚Ä¢ SMOTE (Synthetic Minority Oversampling)
  ‚Ä¢ SMOTEENN (SMOTE + Edited Nearest Neighbours)
  ‚Ä¢ ADASYN (Adaptive Synthetic Sampling)
  ‚Ä¢ RandomUnderSampler
„ÉªSelect optimal balancing method;

note left
**Imbalance Handling:**
‚Ä¢ Compare 4 different techniques
‚Ä¢ Evaluate impact on class distribution
‚Ä¢ Select SMOTE as primary method
‚Ä¢ Maintain stratification in splits
end note

:üîÑ **STEP 7: Train-Test Split**
„ÉªStratified split (75% train, 25% test)
„ÉªMaintain class distribution
„ÉªApply random seed for reproducibility;

:ü§ñ **STEP 8: Model Selection & Training**
„ÉªDefine 10 ML algorithms:
  ‚Ä¢ Decision Tree
  ‚Ä¢ Gradient Boosting Machine
  ‚Ä¢ XGBoost
  ‚Ä¢ Random Forest
  ‚Ä¢ Logistic Regression
  ‚Ä¢ Support Vector Machine
  ‚Ä¢ Neural Network (MLP)
  ‚Ä¢ AdaBoost
  ‚Ä¢ LightGBM
  ‚Ä¢ CatBoost;

:üéØ **STEP 9: Hyperparameter Optimization**
„ÉªApply Bayesian Optimization (Optuna)
„ÉªDefine parameter search spaces
„ÉªOptimize for ROC-AUC score
„ÉªUse time-limited optimization;

note right
**Optimization Strategy:**
‚Ä¢ Bayesian approach using Optuna
‚Ä¢ ROC-AUC as optimization metric
‚Ä¢ Time-limited trials per model
‚Ä¢ Cross-validation during optimization
end note

:üìä **STEP 10: Cross-Validation Evaluation**
„Éª5-fold Stratified Cross-Validation
„ÉªCalculate multiple metrics:
  ‚Ä¢ ROC-AUC
  ‚Ä¢ Accuracy
  ‚Ä¢ Precision
  ‚Ä¢ Recall
  ‚Ä¢ F1-score
„ÉªGenerate performance distributions;

:üìà **STEP 11: Statistical Significance Testing**
„ÉªWilcoxon Signed-Rank Test
„ÉªFriedman Test for multiple comparisons
„ÉªIdentify best performing model
„ÉªCalculate p-values for significance;

note left
**Statistical Tests:**
‚Ä¢ Wilcoxon: pairwise model comparison
‚Ä¢ Friedman: overall model ranking
‚Ä¢ Œ± = 0.05 significance level
‚Ä¢ Multiple comparison correction
end note

:üèÜ **STEP 12: Best Model Selection**
„ÉªSelect model with highest CV ROC-AUC
„ÉªTrain final model on full training set
„ÉªGenerate comprehensive test predictions;

if (Model supports feature importance?) then (yes)
  :üìä **Feature Importance Analysis**
  „ÉªExtract feature importances
  „ÉªRank features by contribution
  „ÉªGenerate importance visualizations;
else (no)
  :‚ö†Ô∏è Skip feature importance;
endif

:üîç **STEP 13: SHAP Analysis**
„ÉªGenerate SHAP explanations
„ÉªCreate summary plots
„ÉªAnalyze feature contributions
„ÉªUnderstand model decisions;

note right
**SHAP Analysis:**
‚Ä¢ TreeExplainer for tree-based models
‚Ä¢ KernelExplainer for other models
‚Ä¢ Global and local explanations
‚Ä¢ Feature interaction analysis
end note

:‚öñÔ∏è **STEP 14: Bias & Fairness Analysis**
„ÉªDemographic Parity Assessment
„ÉªEqual Opportunity Analysis
„ÉªCalculate fairness metrics
„ÉªIdentify potential biases;

note left
**Fairness Metrics:**
‚Ä¢ Demographic parity difference
‚Ä¢ Equalized odds difference
‚Ä¢ True Positive Rate by group
‚Ä¢ Bias detection and mitigation
end note

:üîÑ **STEP 15: Counterfactual Explanations**
„ÉªGenerate counterfactual examples using DICE-ML
„ÉªIdentify minimal changes for outcome flip
„ÉªProvide actionable insights;

:üìã **STEP 16: Model Evaluation**
„ÉªConfusion Matrix Analysis
„ÉªROC Curve Generation
„ÉªPrecision-Recall Curve
„ÉªCalibration Curve Assessment
„ÉªCalculate comprehensive metrics;

:üìä **STEP 17: Results Compilation**
„ÉªGenerate comprehensive Excel report
„ÉªCreate visualization suite (12+ plots)
„ÉªSave model artifacts for deployment
„ÉªDocument methodology and findings;

:üíº **STEP 18: Business Recommendations**
„ÉªTranslate technical results to business insights
„ÉªProvide deployment guidelines
„ÉªSuggest monitoring strategies
„ÉªRecommend business actions;

note right
**Deliverables:**
‚Ä¢ Trained model artifacts
‚Ä¢ Performance evaluation report
‚Ä¢ Feature importance rankings
‚Ä¢ Business recommendation document
‚Ä¢ Deployment-ready pipeline
end note

stop

@enduml